{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db0ad39-d2ef-442a-92ee-3cf4e9c0a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from typing import Literal\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import torch  # noqa: F401\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "import torchvision\n",
    "import transformers.models.llama.modeling_llama\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if \"HUGGINGFACE_TOKEN\" in os.environ:\n",
    "    HfFolder.save_token(os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4db7b5-f418-4368-a915-2de5df709f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"Arena_QS_updated_filtered.csv\", nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3f629e-86a8-4b0d-8094-2d759658d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_.sample(n=10, random_state=42)\n",
    "\n",
    "def sample_questions(\n",
    "    df: pd.DataFrame, question_types: list, sample_qs: int, random_seed: int = 42\n",
    "):\n",
    "    random.seed(random_seed)\n",
    "    mapping = []\n",
    "    prompts = []\n",
    "    sampled_ids = []\n",
    "    for idx, row in df.iterrows():\n",
    "        context = row[\"text\"]\n",
    "        available = [qt for qt in question_types if pd.notna(row.get(qt))]\n",
    "        if not available:\n",
    "            continue\n",
    "        if sample_qs > 0 and len(available) > sample_qs:\n",
    "            chosen = random.sample(available, sample_qs)\n",
    "        else:\n",
    "            chosen = available\n",
    "        for qt in chosen:\n",
    "            prompt = f\"\"\"\n",
    "Context: {context}\n",
    "Question ({qt}): {row[qt]}\n",
    "\n",
    "Answer the question in Kazakh language, use information provided in the context. Be concise and clear, only answer the question asked, but answer it well.\n",
    "\"\"\"\n",
    "            prompts.append(prompt)\n",
    "            mapping.append(\n",
    "                {\n",
    "                    \"task_id\": idx,\n",
    "                    \"question\": qt,\n",
    "                    \"question_type\": qt,\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": prompt,\n",
    "                }\n",
    "            )\n",
    "            sampled_ids.append(f\"{idx}-{qt}\")\n",
    "    return mapping, prompts, sampled_ids\n",
    "\n",
    "\n",
    "mapping, prompts, _ = sample_questions(df, {\"WHY_QS\", \"WHAT_QS\", \"HOW_QS\", \"DESCRIBE_QS\", \"ANALYZE_QS\"}, sample_qs=2, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905b2514-f3d5-4a5d-a7b3-4c885a2e3acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping), len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1b06e-7156-473d-b317-2e827a1df792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead58a47-ce8b-4612-9b98-8c09cdd6d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"armanibadboy/llama3.2-kazllm-3b-by-arman\" # +\n",
    "# model_id = \"meta-llama/Llama-3.2-1B-Instruct\" # +\n",
    "model_id = \"TilQazyna/llama-kaz-instruct-8B-1\"\n",
    "# model_id = \"google/gemma-2-2b-it\"\n",
    "# model_id = \"AmanMussa/llama2-kazakh-7b\"\n",
    "# model_id = \"IrbisAI/Irbis-7b-v0.1\"\n",
    "# model_id = \"armanibadboy/llama3.1-kazllm-8b-by-arman-ver2\"\n",
    "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_id = \"google/gemma-2-9b-it\"\n",
    "# model_id = \"issai/LLama-3.1-KazLLM-1.0-8B\"\n",
    "\n",
    "\n",
    "if model_id == \"armanibadboy/llama3.2-kazllm-3b-by-arman\":\n",
    "    extra = {\n",
    "        \"gguf_file\": \"unsloth.Q8_0.gguf\",\n",
    "    }\n",
    "else:\n",
    "    extra = {}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\", trust_remote_code=True, **extra\n",
    ")\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d354008-61e4-40da-9403-9f08d5ab241a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2d469-2d53-42aa-b9c7-9cd25e0b976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"num_beams\": 1,\n",
    "    \"temperature\": 0.1,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"remove_invalid_values\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"forced_eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"use_cache\": True,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "outputs = []\n",
    "batch_size = 5\n",
    "batch_prompts = prompts[0:batch_size]\n",
    "chat_inputs = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    for prompt in batch_prompts\n",
    "]\n",
    "\n",
    "formatted_inputs = tokenizer.apply_chat_template(\n",
    "    chat_inputs,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "formatted_inputs = formatted_inputs.to(device)\n",
    "attention_masks = []\n",
    "for input_ids in formatted_inputs:\n",
    "    number_of_padding = 0\n",
    "    for token_id in input_ids:\n",
    "        if token_id == tokenizer.pad_token_id:\n",
    "            number_of_padding += 1\n",
    "        else:\n",
    "            break\n",
    "    attention_masks.append(\n",
    "        [0] * number_of_padding + [1] * (len(input_ids) - number_of_padding)\n",
    "    )\n",
    "attention_masks = torch.tensor(attention_masks).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c168e68-2b03-45ce-bcbf-48ffb9623eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab347473-aec6-4ffb-b22d-f99c9a0c9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out_ids = model.generate(\n",
    "        **{\n",
    "            \"input_ids\": formatted_inputs,\n",
    "            \"attention_mask\": attention_masks,\n",
    "        },\n",
    "        **generation_config,\n",
    "    )\n",
    "    if out_ids.ndim == 1:\n",
    "        out_ids = out_ids.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f38d0-cf68-46b6-9200-4c02c0828100",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(batch_prompts)):\n",
    "    input_length = formatted_inputs[j].shape[0]\n",
    "    generated_tokens = out_ids[j][input_length:]\n",
    "    out_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    out_text = out_text[len(\"assistant\") :].strip()\n",
    "    rec = mapping[j]\n",
    "    rec[\"output\"] = out_text\n",
    "    # rec[\"generation_id\"] = str(uuid.uuid4())\n",
    "    outputs.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bf612-c4bc-4b85-836e-dbf850839398",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[4]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f752979-2c25-4f51-a966-5685bacf4b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2951f66-4e97-4ed8-9e95-b7dbe0ada311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadfe0e-28c1-48ad-8e3d-78dccf62f782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b6fbc-37ac-46e0-bdf0-0f483f4d199e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
