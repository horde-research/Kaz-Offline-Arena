{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db0ad39-d2ef-442a-92ee-3cf4e9c0a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from typing import Literal\n",
    "import peft\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import torch  # noqa: F401\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "import torchvision\n",
    "import transformers.models.llama.modeling_llama\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if \"HUGGINGFACE_TOKEN\" in os.environ:\n",
    "    HfFolder.save_token(os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c20d4fb-0e44-4198-948c-20ca7996a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4db7b5-f418-4368-a915-2de5df709f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"Arena_QS_updated_filtered.csv\", nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3f629e-86a8-4b0d-8094-2d759658d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_.sample(n=10, random_state=42)\n",
    "\n",
    "def sample_questions(\n",
    "    df: pd.DataFrame, question_types: list, sample_qs: int, random_seed: int = 42\n",
    "):\n",
    "    random.seed(random_seed)\n",
    "    mapping = []\n",
    "    prompts = []\n",
    "    sampled_ids = []\n",
    "    for idx, row in df.iterrows():\n",
    "        context = row[\"text\"][:12_000] # TODO\n",
    "        available = [qt for qt in question_types if pd.notna(row.get(qt))]\n",
    "        if not available:\n",
    "            continue\n",
    "        if sample_qs > 0 and len(available) > sample_qs:\n",
    "            chosen = random.sample(available, sample_qs)\n",
    "        else:\n",
    "            chosen = available\n",
    "        for qt in chosen:\n",
    "            prompt = f\"\"\"\n",
    "Context: {context}\n",
    "Question ({qt}): {row[qt]}\n",
    "\n",
    "Answer the question in Kazakh language, use information provided in the context. Be concise and clear, only answer the question asked, but answer it well.\n",
    "\"\"\"\n",
    "            prompts.append(prompt)\n",
    "            mapping.append(\n",
    "                {\n",
    "                    \"task_id\": idx,\n",
    "                    \"question\": qt,\n",
    "                    \"question_type\": qt,\n",
    "                    \"context\": context,\n",
    "                    \"prompt\": prompt,\n",
    "                }\n",
    "            )\n",
    "            sampled_ids.append(f\"{idx}-{qt}\")\n",
    "    return mapping, prompts, sampled_ids\n",
    "\n",
    "\n",
    "mapping, prompts, _ = sample_questions(df, {\"WHY_QS\", \"WHAT_QS\", \"HOW_QS\", \"DESCRIBE_QS\", \"ANALYZE_QS\"}, sample_qs=1, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905b2514-f3d5-4a5d-a7b3-4c885a2e3acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping), len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1b06e-7156-473d-b317-2e827a1df792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead58a47-ce8b-4612-9b98-8c09cdd6d917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3dd9bc30ae4682baf6ca4d723dd0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3efb7e546a4fdb9307cb774e8fc131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:  96%|#########6| 4.74G/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1551c391103b442c9a7d8f1a382bebe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56506399a56d436ba9bf125c130a5f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03862dbc56894d6f85e31ab0bef3dbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_id = \"armanibadboy/llama3.2-kazllm-3b-by-arman\" # +\n",
    "# model_id = \"meta-llama/Llama-3.2-1B-Instruct\" # +\n",
    "# model_id = \"TilQazyna/llama-kaz-instruct-8B-1\" # +\n",
    "# model_id = \"google/gemma-2-2b-it\" # +\n",
    "# model_id = \"AmanMussa/llama2-kazakh-7b\" # -\n",
    "# model_id = \"IrbisAI/Irbis-7b-v0.1\" # +\n",
    "# model_id = \"armanibadboy/llama3.1-kazllm-8b-by-arman-ver2\" # -\n",
    "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\" # +\n",
    "# model_id = \"Qwen/Qwen2.5-7B-Instruct\" # +\n",
    "# model_id = \"meta-llama/Llama-3.1-8B-Instruct\" # +\n",
    "# model_id = \"google/gemma-2-9b-it\" # +\n",
    "model_id = \"issai/LLama-3.1-KazLLM-1.0-8B\" # +\n",
    "\n",
    "\n",
    "if model_id == \"armanibadboy/llama3.2-kazllm-3b-by-arman\":\n",
    "    extra = {\n",
    "        \"gguf_file\": \"unsloth.Q8_0.gguf\",\n",
    "    }\n",
    "else:\n",
    "    extra = {}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "if model_id == \"armanibadboy/llama3.1-kazllm-8b-by-arman-ver2\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    model = peft.PeftModel.from_pretrained(model, model_id, safe_serialization=True, torch_dtype=torch.bfloat16)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\", trust_remote_code=True, **extra\n",
    "    )\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d354008-61e4-40da-9403-9f08d5ab241a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee2d469-2d53-42aa-b9c7-9cd25e0b976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"num_beams\": 1,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"remove_invalid_values\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"forced_eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"use_cache\": True,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_return_sequences\": 1,\n",
    "}\n",
    "\n",
    "batch_size = 3\n",
    "batch_prompts = prompts[0:batch_size]\n",
    "chat_inputs = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    for prompt in batch_prompts\n",
    "]\n",
    "\n",
    "formatted_inputs = tokenizer.apply_chat_template(\n",
    "    chat_inputs,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    # chat_template=\"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\"\"\n",
    "    # chat_template=\"\"\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\"\"\"\n",
    "#     chat_template=\"\"\"{% for message in messages %}\n",
    "# Сұрақ: {{ message['content'] | trim }}\n",
    "# Жауап:\n",
    "# {% endfor %}\"\"\"\n",
    ")\n",
    "formatted_inputs = formatted_inputs.to(device)\n",
    "attention_masks = []\n",
    "for input_ids in formatted_inputs:\n",
    "    number_of_padding = 0\n",
    "    for token_id in input_ids:\n",
    "        if token_id == tokenizer.pad_token_id:\n",
    "            number_of_padding += 1\n",
    "        else:\n",
    "            break\n",
    "    attention_masks.append(\n",
    "        [0] * number_of_padding + [1] * (len(input_ids) - number_of_padding)\n",
    "    )\n",
    "attention_masks = torch.tensor(attention_masks).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c168e68-2b03-45ce-bcbf-48ffb9623eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch._dynamo.config.capture_dynamic_output_shape_ops = True\n",
    "# torch._dynamo.config.capture_scalar_outputs = True\n",
    "# torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f6fce89-7c05-4a3c-8d94-fe18ace2b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab347473-aec6-4ffb-b22d-f99c9a0c9cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out_ids = model.generate(\n",
    "        **{\n",
    "            \"input_ids\": formatted_inputs,\n",
    "            \"attention_mask\": attention_masks,\n",
    "        },\n",
    "        **generation_config,\n",
    "    )\n",
    "    if out_ids.ndim == 1:\n",
    "        out_ids = out_ids.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff3f38d0-cf68-46b6-9200-4c02c0828100",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for j in range(len(batch_prompts)):\n",
    "    input_length = formatted_inputs[j].shape[0]\n",
    "    generated_tokens = out_ids[j][input_length:]\n",
    "    out_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    out_text = out_text[len(\"assistant\") :].strip()\n",
    "    # out_text = out_text[len(\"user\\n\\n\") :].strip()\n",
    "    rec = mapping[j]\n",
    "    rec[\"output\"] = out_text\n",
    "    # rec[\"generation_id\"] = str(uuid.uuid4())\n",
    "    outputs.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f81bf612-c4bc-4b85-836e-dbf850839398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Жаңаөзен оқиғасының Қазақстандағы азаматтық қоғамның дамуына әсері теріс болды. Ереуіл және содан кейінгі қантөгіс жұртшылықтың билікке деген сенімін шайқаған. Бұл оқиға сонымен қатар азаматтық қоғамның өзалдық құрылымдарының жемісі емес, саяси биліктердің шаруасы екендігін көрсетті.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f752979-2c25-4f51-a966-5685bacf4b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[0]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2951f66-4e97-4ed8-9e95-b7dbe0ada311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadfe0e-28c1-48ad-8e3d-78dccf62f782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b6fbc-37ac-46e0-bdf0-0f483f4d199e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
